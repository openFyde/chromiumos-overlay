From 6de350e509e9a5bd0417f0b51d3ab31276099217 Mon Sep 17 00:00:00 2001
From: Chia-I Wu <olvaffe@gmail.com>
Date: Tue, 27 Jul 2021 11:21:46 -0700
Subject: [PATCH] vkr: add support for globalFencing

Squashed commit of the following:

commit 77cd29f74c364c43ade11b658c408a7777f7f9fa
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Wed Jul 21 19:01:28 2021 -0700

    virgl: fix another fence export bug

    Track the seqno of the last retired fence.  Return a signaled fd in
    virgl_renderer_export_fence when the requested seqno is less than or
    equal to the seqno of the last retired fence.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit b0fb314c6156117dcd792cb4c482203abf214c5f
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Mon Jul 19 18:54:37 2021 -0700

    vkr: allow more than one logical devices

    Convert errors into warnings.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 2deaf2220ef979f65150a2c6d317c8d3aaaa6249
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Mon Jul 19 18:48:51 2021 -0700

    virgl: virgl_egl_export_signaled_fence returns a bool not int

    Caught by Femi Adegunloye.  Thanks!

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 5e837a8d940389a9860d53daa40f1dc8e018933c
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Thu Jul 8 09:48:26 2021 -0700

    virgl: global fencing with VIRGL_RENDERER_ASYNC_FENCE_CB

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit b4313f826f2454d16a3ec7371b83f413615122f6
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Thu Jul 8 09:55:26 2021 -0700

    virgl: clean up virgl_renderer_poll

    Add a helper, timeline_poll_retired_fences, to poll retired fences.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 648bae08e908dedf0c0521d46427fd00b9533920
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Tue Jun 15 23:54:47 2021 -0700

    vkr: allow multiple logical devices in some cases

    When a logical device does not use external memory/fence/semaphore, we
    know the guest driver does not need queue id 0 to work.  Allow such a
    logical device to be created.

    This fixes vulkaninfo.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit bcbb430974a77bafc52782bba339c714e0b15590
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Tue Jun 8 12:39:00 2021 -0700

    vkr: advertise globalFencing

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 4f8edf45c524de771003e868501efb6bf3c2d7d0
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Tue Mar 16 16:22:18 2021 -0700

    virgl: put virgl and venus on the same fence timeline

    In other words, these functions

      virgl_renderer_create_fence
      virgl_renderer_poll
      virgl_renderer_export_fence
      (but not virgl_renderer_get_poll_fd)

    now work with venus.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 79881ff012887de9124aa1c6b1fc21566fd717d6
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Tue Mar 16 16:50:02 2021 -0700

    vkr: add support for queue_id 0

    Becase we advertise only a single VkQueue per-context, we can treat
    queue id 0 as the single VkQueue.  When the queue hasn't been created,
    all fences are treated as cpu fences.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 7c2b8de1f32c5108be1120c3bbb4914eef715a3c
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Fri Jun 11 14:33:44 2021 -0700

    vkr: implement virgl_context::export_fence

    This assumes there is only a single VkQueue, which can be relaxed if we
    choose to.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 1d19bc4a7d5d00aabaaf6d7f9a7eae41ad99c79b
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Fri Jun 11 14:41:52 2021 -0700

    vkr: restrict to a single VkQueue per-context

    This simplifies things when venus joins global fencing.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 10d2233a26f8c523216d458b80bddac0d238e6a8
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Tue Mar 16 16:21:35 2021 -0700

    virgl: pass fence flags in fence retire callbacks

    This allows us to set internal flags and check for them in the
    retire callbacks.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>

commit 532ba315b88ec55a4ff3ffb6415582a9a061d3da
Author: Chia-I Wu <olvaffe@gmail.com>
Date:   Fri Jun 4 12:24:02 2021 -0700

    virgl: add virgl_context::export_fence

    This is needed when we get per-context version of
    virgl_renderer_export_fence.

    Signed-off-by: Chia-I Wu <olvaffe@gmail.com>
    Reviewed-by: Yiwei Zhang <zzyiwei@chromium.org>
---
 src/virgl_context.h  |   6 +
 src/virglrenderer.c  | 272 +++++++++++++++++++++++++++++++++++++++++--
 src/vkr_renderer.c   | 171 ++++++++++++++++++++++++++-
 src/vrend_decode.c   |   6 +-
 src/vrend_renderer.c |   4 +-
 src/vrend_renderer.h |   3 +-
 6 files changed, 447 insertions(+), 15 deletions(-)

diff --git a/src/virgl_context.h b/src/virgl_context.h
index ea86b31..871a148 100644
--- a/src/virgl_context.h
+++ b/src/virgl_context.h
@@ -51,6 +51,7 @@ struct virgl_context_blob {
 struct virgl_context;
 
 typedef void (*virgl_context_fence_retire)(struct virgl_context *ctx,
+                                           uint32_t flags,
                                            uint64_t queue_id,
                                            void *fence_cookie);
 
@@ -120,6 +121,11 @@ struct virgl_context {
                        uint32_t flags,
                        uint64_t queue_id,
                        void *fence_cookie);
+
+   /* export the fence identified by fence_cookie as a sync fd */
+   int (*export_fence)(struct virgl_context *ctx,
+                       void *fence_cookie,
+                       int *out_fd);
 };
 
 struct virgl_context_foreach_args {
diff --git a/src/virglrenderer.c b/src/virglrenderer.c
index 7250c1d..ef15562 100644
--- a/src/virglrenderer.c
+++ b/src/virglrenderer.c
@@ -33,6 +33,7 @@
 #include <unistd.h>
 
 #include "pipe/p_state.h"
+#include "util/u_double_list.h"
 #include "util/u_format.h"
 #include "util/u_math.h"
 #include "vkr_renderer.h"
@@ -46,6 +47,20 @@
 #include "virgl_resource.h"
 #include "virgl_util.h"
 
+#include "c11/threads.h"
+
+#define VIRGL_RENDERER_FENCE_FLAG_TIMELINE (1u << 31)
+
+struct timeline_point {
+   uint32_t fence_id;
+
+   bool signaled;
+   /* can be NULL if has signaled or is using ctx0 fencing */
+   struct virgl_context *context;
+
+   struct list_head head;
+};
+
 struct global_state {
    bool client_initialized;
    void *cookie;
@@ -57,6 +72,16 @@ struct global_state {
    bool winsys_initialized;
    bool vrend_initialized;
    bool vkr_initialized;
+
+   bool timeline_async_cb;
+   mtx_t timeline_mutex;
+   /* when timeline_async_cb is true, these can be accessed by the main thread
+    * and the sync threads simultaneously and are protected by timeline_mutex
+    */
+   struct list_head timeline;
+   uint32_t timeline_retired_fence_id;
+   uint32_t ctx0_retired_fence_id;
+   struct list_head free_points;
 };
 
 static struct global_state state;
@@ -175,10 +200,114 @@ void virgl_renderer_fill_caps(uint32_t set, uint32_t version,
    }
 }
 
+static void timeline_lock(void)
+{
+   /* no check for state.timeline_async_cb because this should be cheap
+    * (cheaper than the if-check?) in the non-contended case
+    */
+   mtx_lock(&state.timeline_mutex);
+}
+
+static void timeline_unlock(void)
+{
+   mtx_unlock(&state.timeline_mutex);
+}
+
+static struct timeline_point *timeline_point_alloc(uint32_t fence_id,
+                                                   struct virgl_context *ctx)
+{
+   struct timeline_point *point;
+
+   timeline_lock();
+   if (LIST_IS_EMPTY(&state.free_points)) {
+      timeline_unlock();
+
+      point = malloc(sizeof(*point));
+      if (!point)
+         return NULL;
+   } else {
+      point = LIST_ENTRY(struct timeline_point, state.free_points.next, head);
+      list_del(&point->head);
+
+      timeline_unlock();
+   }
+
+   point->fence_id = fence_id;
+   point->signaled = false;
+   point->context = ctx;
+
+   return point;
+}
+
+static void timeline_point_add_locked(struct timeline_point *point)
+{
+   list_addtail(&point->head, &state.timeline);
+}
+
+static void timeline_point_free_locked(struct timeline_point *point)
+{
+   list_add(&point->head, &state.free_points);
+}
+
+static bool timeline_point_match_context_locked(const struct timeline_point *point,
+                                                uint32_t ctx_id)
+{
+   return point->context && point->context->ctx_id == ctx_id;
+}
+
+static void timeline_point_set_signaled_locked(struct timeline_point *point)
+{
+   point->signaled = true;
+   point->context = NULL;
+}
+
+static bool timeline_point_is_signaled_locked(const struct timeline_point *point)
+{
+   return point->signaled ||
+          (!point->context && point->fence_id <= state.ctx0_retired_fence_id);
+}
+
+static uint32_t timeline_poll_retired_fences_locked(void)
+{
+   uint32_t write_fence_id = 0;
+   struct timeline_point *point, *tmp;
+   LIST_FOR_EACH_ENTRY_SAFE(point, tmp, &state.timeline, head) {
+      if (!timeline_point_is_signaled_locked(point))
+         break;
+
+      write_fence_id = point->fence_id;
+      list_del(&point->head);
+      timeline_point_free_locked(point);
+   }
+
+   if (write_fence_id)
+      state.timeline_retired_fence_id = write_fence_id;
+
+   return write_fence_id;
+}
+
 static void per_context_fence_retire(struct virgl_context *ctx,
+                                     uint32_t flags,
                                      uint64_t queue_id,
                                      void *fence_cookie)
 {
+   if (flags & VIRGL_RENDERER_FENCE_FLAG_TIMELINE) {
+      if (state.timeline_async_cb) {
+         uint32_t write_fence_id = 0;
+         timeline_lock();
+         timeline_point_set_signaled_locked(fence_cookie);
+         write_fence_id = timeline_poll_retired_fences_locked();
+         timeline_unlock();
+
+         if (write_fence_id)
+            state.cbs->write_fence(state.cookie, write_fence_id);
+      } else {
+         timeline_point_set_signaled_locked(fence_cookie);
+      }
+
+      return;
+   }
+
    state.cbs->write_context_fence(state.cookie,
                                   ctx->ctx_id,
                                   queue_id,
@@ -253,6 +382,16 @@ int virgl_renderer_context_create(uint32_t handle, uint32_t nlen, const char *na
 void virgl_renderer_context_destroy(uint32_t handle)
 {
    TRACE_FUNC();
+
+   struct timeline_point *point;
+
+   timeline_lock();
+   LIST_FOR_EACH_ENTRY(point, &state.timeline, head) {
+      if (timeline_point_match_context_locked(point, handle))
+         timeline_point_set_signaled_locked(point);
+   }
+   timeline_unlock();
+
    virgl_context_remove(handle);
 }
 
@@ -379,13 +518,46 @@ void virgl_renderer_resource_detach_iov(int res_handle, struct iovec **iov_p, in
    virgl_resource_detach_iov(res);
 }
 
-int virgl_renderer_create_fence(int client_fence_id, UNUSED uint32_t ctx_id)
+int virgl_renderer_create_fence(int client_fence_id, uint32_t ctx_id)
 {
    TRACE_FUNC();
    const uint32_t fence_id = (uint32_t)client_fence_id;
-   if (state.vrend_initialized)
-      return vrend_renderer_create_ctx0_fence(fence_id);
-   return EINVAL;
+
+   struct virgl_context *ctx;
+   struct timeline_point *point;
+   int ret;
+
+   /* this only works with crosvm because qemu passes garbage for ctx_id */
+   if (ctx_id) {
+      ctx = virgl_context_lookup(ctx_id);
+      if (!ctx)
+         return -EINVAL;
+      /* use per-context fencing only for venus */
+      if (ctx->capset_id != VIRGL_RENDERER_CAPSET_VENUS)
+         ctx = NULL;
+   } else {
+      ctx = NULL;
+   }
+
+   point = timeline_point_alloc(fence_id, ctx);
+   if (!point)
+      return -ENOMEM;
+
+   if (ctx) {
+      ret = ctx->submit_fence(ctx, VIRGL_RENDERER_FENCE_FLAG_TIMELINE, 0, point);
+   } else {
+      ret = state.vrend_initialized ?
+         vrend_renderer_create_ctx0_fence(fence_id) : EINVAL;
+   }
+
+   timeline_lock();
+   if (ret)
+      timeline_point_free_locked(point);
+   else
+      timeline_point_add_locked(point);
+   timeline_unlock();
+
+   return ret;
 }
 
 int virgl_renderer_context_create_fence(uint32_t ctx_id,
@@ -505,11 +677,25 @@ void virgl_renderer_get_rect(int resource_id, struct iovec *iov, unsigned int nu
 }
 
 
-static void ctx0_fence_retire(void *fence_cookie,
+static void ctx0_fence_retire(UNUSED uint32_t flags,
+                              void *fence_cookie,
                               UNUSED void *retire_data)
 {
    const uint32_t fence_id = (uint32_t)(uintptr_t)fence_cookie;
-   state.cbs->write_fence(state.cookie, fence_id);
+
+   if (state.timeline_async_cb) {
+      uint32_t write_fence_id = 0;
+      timeline_lock();
+      state.ctx0_retired_fence_id = fence_id;
+      write_fence_id = timeline_poll_retired_fences_locked();
+      timeline_unlock();
+
+      if (write_fence_id)
+         state.cbs->write_fence(state.cookie, write_fence_id);
+   } else {
+      /* defer marking timeline_point signaled */
+      state.ctx0_retired_fence_id = fence_id;
+   }
 }
 
 static virgl_renderer_gl_context create_gl_context(int scanout_idx, struct virgl_gl_ctx_param *param)
@@ -563,11 +749,33 @@ void *virgl_renderer_get_cursor_data(uint32_t resource_id, uint32_t *width, uint
                                              height);
 }
 
+static bool timeline_poll(struct virgl_context *ctx, UNUSED void *data)
+{
+   /* we use per-context fencing only for venus */
+   if (ctx->capset_id == VIRGL_RENDERER_CAPSET_VENUS)
+      ctx->retire_fences(ctx);
+   return true;
+}
+
 void virgl_renderer_poll(void)
 {
    TRACE_FUNC();
+
+   if (state.timeline_async_cb)
+      return;
+
    if (state.vrend_initialized)
       vrend_renderer_check_fences();
+
+   struct virgl_context_foreach_args args;
+   args.callback = timeline_poll;
+   args.data = NULL;
+   virgl_context_foreach(&args);
+
+   /* no locking needed because state.timeline_async_cb is false */
+   const uint32_t write_fence_id = timeline_poll_retired_fences_locked();
+   if (write_fence_id)
+      state.cbs->write_fence(state.cookie, write_fence_id);
 }
 
 void virgl_renderer_cleanup(UNUSED void *cookie)
@@ -691,6 +899,13 @@ int virgl_renderer_init(void *cookie, int flags, struct virgl_renderer_callbacks
       state.vkr_initialized = true;
    }
 
+#ifdef VIRGL_RENDERER_ASYNC_FENCE_CB
+   state.timeline_async_cb = flags & VIRGL_RENDERER_ASYNC_FENCE_CB;
+#endif
+   mtx_init(&state.timeline_mutex, mtx_plain);
+   list_inithead(&state.timeline);
+   list_inithead(&state.free_points);
+
    return 0;
 
 fail:
@@ -965,9 +1180,52 @@ virgl_renderer_resource_export_blob(uint32_t res_id, uint32_t *fd_type, int *fd)
    return 0;
 }
 
+static int
+export_signaled_fence(int *fd)
+{
+#ifdef HAVE_EPOXY_EGL_H
+   if (virgl_egl_supports_fences(egl))
+      return virgl_egl_export_signaled_fence(egl, fd) ? 0 : -EINVAL;
+#endif
+   return -1;
+}
+
 int
 virgl_renderer_export_fence(uint32_t client_fence_id, int *fd)
 {
    TRACE_FUNC();
-   return vrend_renderer_export_ctx0_fence(client_fence_id, fd);
+
+   int ret;
+
+   timeline_lock();
+   if (state.timeline_retired_fence_id >= client_fence_id ||
+       LIST_IS_EMPTY(&state.timeline)) {
+      ret = 0;
+      *fd = -1;
+   } else {
+      struct timeline_point *point;
+
+      ret = -EINVAL;
+      LIST_FOR_EACH_ENTRY(point, &state.timeline, head) {
+         if (point->fence_id != client_fence_id)
+            continue;
+
+         if (timeline_point_is_signaled_locked(point)) {
+            ret = 0;
+            *fd = -1;
+         } else if (point->context) {
+            ret = point->context->export_fence(point->context, point, fd);
+         } else {
+            ret = vrend_renderer_export_ctx0_fence(client_fence_id, fd);
+         }
+         break;
+      }
+   }
+   timeline_unlock();
+
+   /* required by crosvm */
+   if (!ret && *fd == -1)
+      ret = export_signaled_fence(fd);
+
+   return ret;
 }
diff --git a/src/vkr_renderer.c b/src/vkr_renderer.c
index 3650e47..398f748 100644
--- a/src/vkr_renderer.c
+++ b/src/vkr_renderer.c
@@ -213,6 +213,8 @@ struct vkr_physical_device {
    bool EXT_external_memory_dma_buf;
 
    bool KHR_external_fence_fd;
+
+   struct vkr_device *queue_id_0_device;
 };
 
 struct vkr_queue_sync {
@@ -455,6 +457,9 @@ struct vkr_context {
    int fence_eventfd;
    struct list_head busy_queues;
 
+   struct vkr_queue *queue_id_0_queue;
+   struct list_head cpu_syncs;
+
    struct vkr_instance *instance;
 };
 
@@ -1094,6 +1099,9 @@ vkr_instance_enumerate_physical_devices(struct vkr_instance *instance)
    if (result != VK_SUCCESS)
       return result;
 
+   /* enumerate at most 1 physical device */
+   count = 1;
+
    VkPhysicalDevice *handles = calloc(count, sizeof(*handles));
    struct vkr_physical_device **physical_devs = calloc(count, sizeof(*physical_devs));
    if (!handles || !physical_devs) {
@@ -1103,6 +1111,8 @@ vkr_instance_enumerate_physical_devices(struct vkr_instance *instance)
    }
 
    result = vkEnumeratePhysicalDevices(instance->base.handle.instance, &count, handles);
+   if (result == VK_INCOMPLETE)
+      result = VK_SUCCESS;
    if (result != VK_SUCCESS) {
       free(physical_devs);
       free(handles);
@@ -1416,6 +1426,12 @@ vkr_dispatch_vkGetPhysicalDeviceQueueFamilyProperties(
    vkGetPhysicalDeviceQueueFamilyProperties(args->physicalDevice,
                                             args->pQueueFamilyPropertyCount,
                                             args->pQueueFamilyProperties);
+
+   if (*args->pQueueFamilyPropertyCount) {
+      *args->pQueueFamilyPropertyCount = 1;
+      if (args->pQueueFamilyProperties)
+         args->pQueueFamilyProperties->queueCount = 1;
+   }
 }
 
 static void
@@ -1543,6 +1559,12 @@ vkr_dispatch_vkGetPhysicalDeviceQueueFamilyProperties2(
    vkGetPhysicalDeviceQueueFamilyProperties2(args->physicalDevice,
                                              args->pQueueFamilyPropertyCount,
                                              args->pQueueFamilyProperties);
+
+   if (*args->pQueueFamilyPropertyCount) {
+      *args->pQueueFamilyPropertyCount = 1;
+      if (args->pQueueFamilyProperties)
+         args->pQueueFamilyProperties->queueFamilyProperties.queueCount = 1;
+   }
 }
 
 static void
@@ -1757,7 +1779,8 @@ vkr_queue_thread(void *arg)
       list_del(&sync->head);
 
       if (vkr_renderer_flags & VKR_RENDERER_ASYNC_FENCE_CB) {
-         ctx->base.fence_retire(&ctx->base, queue->base.id, sync->fence_cookie);
+         ctx->base.fence_retire(&ctx->base, sync->flags, queue->base.id,
+                                sync->fence_cookie);
          vkr_device_free_queue_sync(queue->device, sync);
       } else {
          list_addtail(&sync->head, &queue->signaled_syncs);
@@ -1796,6 +1819,8 @@ vkr_queue_destroy(struct vkr_context *ctx, struct vkr_queue *queue)
 
    list_del(&queue->head);
    list_del(&queue->busy_head);
+   if (ctx->queue_id_0_queue == queue)
+      ctx->queue_id_0_queue = NULL;
 
    util_hash_table_remove_u64(ctx->object_table, queue->base.id);
 }
@@ -1857,6 +1882,8 @@ vkr_queue_create(struct vkr_context *ctx,
 
    list_addtail(&queue->head, &dev->queues);
    list_inithead(&queue->busy_head);
+   if (dev->physical_device->queue_id_0_device == dev)
+      ctx->queue_id_0_queue = queue;
 
    util_hash_table_set_u64(ctx->object_table, queue->base.id, queue);
 
@@ -1876,6 +1903,25 @@ vkr_dispatch_vkCreateDevice(struct vn_dispatch_context *dispatch,
       return;
    }
 
+   /* when external memory/fence/semaphore is enabled, the guest driver
+    * expects queue id 0 to be the queue of this device
+    */
+   bool use_queue_id_0 = false;
+   for (uint32_t i = 0; i < args->pCreateInfo->enabledExtensionCount; i++) {
+      if (!strcmp(args->pCreateInfo->ppEnabledExtensionNames[i],
+                  VK_KHR_EXTERNAL_MEMORY_FD_EXTENSION_NAME) ||
+          !strcmp(args->pCreateInfo->ppEnabledExtensionNames[i],
+                  VK_KHR_EXTERNAL_FENCE_FD_EXTENSION_NAME) ||
+          !strcmp(args->pCreateInfo->ppEnabledExtensionNames[i],
+                  VK_KHR_EXTERNAL_SEMAPHORE_FD_EXTENSION_NAME))
+         use_queue_id_0 = true;
+   }
+
+   if (use_queue_id_0 && physical_dev->queue_id_0_device) {
+      vrend_printf("more than one logical device with external memory/fence/semaphore enabled\n");
+      vrend_printf("external fencing might not work\n");
+   }
+
    /* append extensions for our own use */
    const char **exts = NULL;
    uint32_t ext_count = args->pCreateInfo->enabledExtensionCount;
@@ -2019,6 +2065,9 @@ vkr_dispatch_vkCreateDevice(struct vn_dispatch_context *dispatch,
    list_inithead(&dev->free_syncs);
 
    util_hash_table_set_u64(ctx->object_table, dev->base.id, dev);
+
+   if (use_queue_id_0 && !physical_dev->queue_id_0_device)
+      physical_dev->queue_id_0_device = dev;
 }
 
 static void
@@ -2050,6 +2099,9 @@ vkr_dispatch_vkDestroyDevice(struct vn_dispatch_context *dispatch,
    vn_replace_vkDestroyDevice_args_handle(args);
    vkDestroyDevice(args->device, NULL);
 
+   if (dev->physical_device->queue_id_0_device == dev)
+      dev->physical_device->queue_id_0_device = NULL;
+
    util_hash_table_remove_u64(ctx->object_table, dev->base.id);
 }
 
@@ -4000,6 +4052,7 @@ vkr_dispatch_vkGetVenusExperimentalFeatureData100000MESA(
 
    const VkVenusExperimentalFeatures100000MESA features = {
       .memoryResourceAllocationSize = VK_TRUE,
+      .globalFencing = VK_TRUE,
    };
 
    vn_replace_vkGetVenusExperimentalFeatureData100000MESA_args_handle(args);
@@ -4326,7 +4379,29 @@ vkr_context_submit_fence_locked(struct virgl_context *base,
    struct vkr_queue *queue;
    VkResult result;
 
-   queue = util_hash_table_get_u64(ctx->object_table, queue_id);
+   if (queue_id) {
+      queue = util_hash_table_get_u64(ctx->object_table, queue_id);
+   } else if (ctx->queue_id_0_queue) {
+      queue = ctx->queue_id_0_queue;
+   } else if (vkr_renderer_flags & VKR_RENDERER_ASYNC_FENCE_CB) {
+      ctx->base.fence_retire(&ctx->base, flags, 0, fence_cookie);
+      return 0;
+   } else {
+      struct vkr_queue_sync *sync = malloc(sizeof(*sync));
+      if (!sync)
+         return -ENOMEM;
+
+      sync->fence = VK_NULL_HANDLE;
+      sync->flags = flags;
+      sync->fence_cookie = fence_cookie;
+      list_addtail(&sync->head, &ctx->cpu_syncs);
+
+      if (ctx->fence_eventfd >= 0)
+         write_eventfd(ctx->fence_eventfd, 1);
+
+      return 0;
+   }
+
    if (!queue)
       return -EINVAL;
    struct vkr_device *dev = queue->device;
@@ -4371,6 +4446,82 @@ vkr_context_submit_fence(struct virgl_context *base,
    return ret;
 }
 
+static struct vkr_queue_sync *
+find_sync(const struct list_head *syncs, void *fence_cookie)
+{
+   struct vkr_queue_sync *sync;
+   LIST_FOR_EACH_ENTRY (sync, syncs, head) {
+      if (sync->fence_cookie == fence_cookie)
+         return sync;
+   }
+   return NULL;
+}
+
+static int
+vkr_context_export_fence_locked(struct virgl_context *base,
+                                void *fence_cookie,
+                                int *out_fd)
+{
+   struct vkr_context *ctx = (struct vkr_context *)base;
+
+   struct vkr_queue_sync *sync = NULL;
+   bool sync_pending = false;
+   if (ctx->queue_id_0_queue) {
+      struct vkr_queue *queue = ctx->queue_id_0_queue;
+
+      if (vkr_renderer_flags & VKR_RENDERER_THREAD_SYNC) {
+         mtx_lock(&queue->mutex);
+         sync = find_sync(&queue->signaled_syncs, fence_cookie);
+      }
+
+      if (!sync) {
+         sync = find_sync(&queue->pending_syncs, fence_cookie);
+         if (sync)
+            sync_pending = true;
+      }
+
+      if (vkr_renderer_flags & VKR_RENDERER_THREAD_SYNC)
+         mtx_unlock(&queue->mutex);
+   }
+
+   if (!sync)
+      sync = find_sync(&ctx->cpu_syncs, fence_cookie);
+
+   if (!sync)
+      return -EINVAL;
+
+   if (!sync_pending) {
+      *out_fd = -1;
+      return 0;
+   }
+
+   struct vkr_device *dev = ctx->queue_id_0_queue->device;
+   if (!dev->physical_device->KHR_external_fence_fd)
+      return -1;
+
+   const VkFenceGetFdInfoKHR get_fd_info = {
+      .sType = VK_STRUCTURE_TYPE_FENCE_GET_FD_INFO_KHR,
+      .fence = sync->fence,
+      .handleType = VK_EXTERNAL_FENCE_HANDLE_TYPE_SYNC_FD_BIT,
+   };
+   VkResult result =
+      ctx->instance->get_fence_fd(dev->base.handle.device, &get_fd_info, out_fd);
+
+   return result == VK_SUCCESS ? 0 : -1;
+}
+
+static int
+vkr_context_export_fence(struct virgl_context *base, void *fence_cookie, int *out_fd)
+{
+   struct vkr_context *ctx = (struct vkr_context *)base;
+   int ret;
+
+   mtx_lock(&ctx->mutex);
+   ret = vkr_context_export_fence_locked(base, fence_cookie, out_fd);
+   mtx_unlock(&ctx->mutex);
+   return ret;
+}
+
 static void
 vkr_context_retire_fences_locked(UNUSED struct virgl_context *base)
 {
@@ -4380,6 +4531,12 @@ vkr_context_retire_fences_locked(UNUSED struct virgl_context *base)
 
    assert(!(vkr_renderer_flags & VKR_RENDERER_ASYNC_FENCE_CB));
 
+   LIST_FOR_EACH_ENTRY_SAFE (sync, sync_tmp, &ctx->cpu_syncs, head) {
+      ctx->base.fence_retire(&ctx->base, sync->flags, 0, sync->fence_cookie);
+      list_del(&sync->head);
+      free(sync);
+   }
+
    /* flush first and once because the per-queue sync threads might write to
     * it any time
     */
@@ -4394,7 +4551,8 @@ vkr_context_retire_fences_locked(UNUSED struct virgl_context *base)
       vkr_queue_retire_syncs(queue, &retired_syncs, &queue_empty);
 
       LIST_FOR_EACH_ENTRY_SAFE (sync, sync_tmp, &retired_syncs, head) {
-         ctx->base.fence_retire(&ctx->base, queue->base.id, sync->fence_cookie);
+         ctx->base.fence_retire(&ctx->base, sync->flags, queue->base.id,
+                                sync->fence_cookie);
          vkr_device_free_queue_sync(dev, sync);
       }
 
@@ -4734,6 +4892,10 @@ vkr_context_destroy(struct virgl_context *base)
    util_hash_table_destroy(ctx->resource_table);
    util_hash_table_destroy_u64(ctx->object_table);
 
+   struct vkr_queue_sync *sync, *tmp;
+   LIST_FOR_EACH_ENTRY_SAFE (sync, tmp, &ctx->cpu_syncs, head)
+      free(sync);
+
    if (ctx->fence_eventfd >= 0)
       close(ctx->fence_eventfd);
 
@@ -4758,6 +4920,7 @@ vkr_context_init_base(struct vkr_context *ctx)
    ctx->base.get_fencing_fd = vkr_context_get_fencing_fd;
    ctx->base.retire_fences = vkr_context_retire_fences;
    ctx->base.submit_fence = vkr_context_submit_fence;
+   ctx->base.export_fence = vkr_context_export_fence;
 }
 
 static void
@@ -4842,6 +5005,8 @@ vkr_context_create(size_t debug_len, const char *debug_name)
 
    list_inithead(&ctx->busy_queues);
 
+   list_inithead(&ctx->cpu_syncs);
+
    return &ctx->base;
 
 fail:
diff --git a/src/vrend_decode.c b/src/vrend_decode.c
index c25a565..40a2d63 100644
--- a/src/vrend_decode.c
+++ b/src/vrend_decode.c
@@ -1479,11 +1479,12 @@ static int vrend_decode_pipe_resource_set_type(struct vrend_context *ctx, const
 static void vrend_decode_ctx_init_base(struct vrend_decode_ctx *dctx,
                                        uint32_t ctx_id);
 
-static void vrend_decode_ctx_fence_retire(void *fence_cookie,
+static void vrend_decode_ctx_fence_retire(uint32_t flags,
+                                          void *fence_cookie,
                                           void *retire_data)
 {
    struct vrend_decode_ctx *dctx = retire_data;
-   dctx->base.fence_retire(&dctx->base, 0, fence_cookie);
+   dctx->base.fence_retire(&dctx->base, flags, 0, fence_cookie);
 }
 
 struct virgl_context *vrend_renderer_context_create(uint32_t handle,
@@ -1762,4 +1763,5 @@ static void vrend_decode_ctx_init_base(struct vrend_decode_ctx *dctx,
    ctx->get_fencing_fd = vrend_decode_ctx_get_fencing_fd;
    ctx->retire_fences = vrend_decode_ctx_retire_fences;
    ctx->submit_fence = vrend_decode_ctx_submit_fence;
+   ctx->export_fence = NULL;
 }
diff --git a/src/vrend_renderer.c b/src/vrend_renderer.c
index fc767a2..f95ec15 100644
--- a/src/vrend_renderer.c
+++ b/src/vrend_renderer.c
@@ -6111,7 +6111,7 @@ static void wait_sync(struct vrend_fence *fence)
    pipe_mutex_unlock(vrend_state.fence_mutex);
 
    if (vrend_state.use_async_fence_cb) {
-      ctx->fence_retire(fence->fence_cookie, ctx->fence_retire_data);
+      ctx->fence_retire(fence->flags, fence->fence_cookie, ctx->fence_retire_data);
       free_fence_locked(fence);
       return;
    }
@@ -9555,7 +9555,7 @@ void vrend_renderer_check_fences(void)
 
    LIST_FOR_EACH_ENTRY_SAFE(fence, stor, &retired_fences, fences) {
       struct vrend_context *ctx = fence->ctx;
-      ctx->fence_retire(fence->fence_cookie, ctx->fence_retire_data);
+      ctx->fence_retire(fence->flags, fence->fence_cookie, ctx->fence_retire_data);
 
       free_fence_locked(fence);
    }
diff --git a/src/vrend_renderer.h b/src/vrend_renderer.h
index ac4031b..be2f225 100644
--- a/src/vrend_renderer.h
+++ b/src/vrend_renderer.h
@@ -111,7 +111,8 @@ struct vrend_format_table {
    uint32_t flags;
 };
 
-typedef void (*vrend_context_fence_retire)(void *fence_cookie,
+typedef void (*vrend_context_fence_retire)(uint32_t flags,
+                                           void *fence_cookie,
                                            void *retire_data);
 
 struct vrend_if_cbs {
-- 
2.32.0.554.ge1b32706d8-goog

